---
title: 个性化推荐系统笔记
date: 2018-06-04 18:49:57
tags: [推荐]
categories: [深度学习整理总结]
---
## 推荐系统原理
***
## 程序解析
### 数据处理部分
#### 导入的包
```python
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from collections import Counter
import tensorflow as tf
import os
import pickle
import re
from tensorflow.python.ops import math_ops
from urllib.request import urlretrieve
from os.path import isfile, isdir
from tqdm import tqdm
import zipfile
import hashlib
```

#### 数据下载
1) `os.path.join(,)`将路径拼接起来；
2) `os.path.exists()`判断路径是否存在；
3) `print('Found {} Data'.format(database_name))`字符串占位符写法
4) `os.makedirs()`创建目录；`shutil.rmtree`删除目录
5) `with...as...`处理上下文环境产生的异常。**工作过程：** 紧跟`with`后面的语句被求值后，返回对象的`__enter__()`方法被调用，这个方法的返回值将被赋值给`as`后面的变量。当`with`后面的代码块全部被执行完之后，将调用前面返回对象的`__exit__()`方法。
6) `DLProgress(tqdm)` 显示下载进度条的格式
7) `urlretrieve(url, filename=None, reporthook=None, data=None)`
	- 参数`finename`指定了保存本地路径(如果参数未指定，`urllib`会生成一个临时文件保存数据)
	- 参数`reporthook`是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。
	- 参数`data`指post到服务器的数据，该方法返回一个包含两个元素的(filename, headers)元组，filename表示保存到本地的路径，header表示服务器的响应头。

8) `assert` 没完善一个程序之前，我们不知道程序在哪里会出错，与其让它在运行最崩溃，不如在出现错误条件时就崩溃，这时候就需要`assert`断言。其是声明其布尔值必须为真的判定，如果发生异常就说明表达示为假。可以理解`assert`断言语句为`raise-if-not`，用来测试表示式，其返回值为假，就会触发异常。**assert的异常参数**，其实就是在 **断言表达式后添加字符串信息**，用来解释断言并更好的知道是哪里出了问题。
9) `hashlib.md5(open(save_path, 'rb').read()).hexdigest()`打开文件并读取文件内容，通过`hashlib.md5`进行加密，`hexdigest()`获取加密字符串。
10) `zipfile.ZipFile(filename,'r')`读取zip文件；`zipfile.ZipFile.extractall`解压zip文件

```python
def _unzip(save_path, _, database_name, data_path):
	print('Extracting {}...'.format(database_name))
	with zipfile.ZipFile(save_path) as zf:
		zf.extractall(data_path)
def download_extract(database_name, data_path):
	DATASET_ML1M='ml-1m'
	if database_name==DATASET_ML1M:
		url='http://files.grouplens.org/datasets/movielens/ml-1m.zip'
		hashcode='c4d9eecfca2ab87c1945afe126590906'
		extract_path=os.path.join(data_path,'ml-1m')
		save_path=os.path.join(data_path,'ml-1m.zip')
		extract_fn=_unzip
	if os.path.exists(extract_path):
		print('Found {} Data'.format(database_name))
		return
	if not os.path.exists(data_path):
		os.makedirs(data_path)
	if not os.path.exists(save_path):
		with DLProgress(unit='B',unit_scale=True,miniters=1,desc='Downloading {}'.format(database_name)) as pbar:
			urlretrieve(
				url,
				save_path,
				pbar.hook)
	assert hashlib.md5(open(save_path,'rb').read()).hexdigest()==hashcode,\
		'{} file is corrupted. Remove the file and try again.'.format(save_path)
	os.makedirs(extract_path)
	try:
		extract_fn(save_path,extract_path,database_name,data_path)
	except Exception as err:
		shutil.rmtree(extract_path)
		raise err
	print('Done.')
class DLProgress(tqdm):
	last_block=0
	def hook(self,block_num=1,block_size=1,total_size=None):
		self.total=total_size
		self.update((block_num-self.last_block)*block_size)
		self.last_block=block_num
```
#### 数据预处理
1. `pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')` sep:分隔符；header：列名；names：结合header=None用于结果的列名列表。
2. `users.filter(regex='UserID|Gender|Age|JobID')`过滤掉不符合条件，返回符合条件的数据。
3. `users.values`原始数据；`users['Gender'].map(gender_map)`映射
4. `enumerate`对于一个可迭代的(iterable)/可遍历的对象(如列表、字符串)，`enumerate`将其组成一个索引序列，利用它可以同时获得索引和值
5. `pattern = re.compile(r'^(.*)\((\d+)\)$')`、`pattern.match(val).group(1)`返回匹配后的第一组数据。
6.


```python
def load_data():

	#读取User数据
	users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']
	users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')
	users = users.filter(regex='UserID|Gender|Age|JobID')
	users_orig = users.values
	#改变User数据中性别和年龄
	gender_map = {'F':0, 'M':1}
	users['Gender'] = users['Gender'].map(gender_map)

	age_map = {val:ii for ii,val in enumerate(set(users['Age']))}
	users['Age'] = users['Age'].map(age_map)

	#读取Movie数据集
	movies_title = ['MovieID', 'Title', 'Genres']
	movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')
	movies_orig = movies.values
	#将Title中的年份去掉
	pattern = re.compile(r'^(.*)\((\d+)\)$')

	title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}
	movies['Title'] = movies['Title'].map(title_map)

	#电影类型转数字字典
	genres_set = set()
	for val in movies['Genres'].str.split('|'):
		genres_set.update(val)

	genres_set.add('<PAD>')
	genres2int = {val:ii for ii, val in enumerate(genres_set)}

	#将电影类型转成等长数字列表，长度是18
	genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}

	for key in genres_map:
		for cnt in range(max(genres2int.values()) - len(genres_map[key])):
			genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])

	movies['Genres'] = movies['Genres'].map(genres_map)

	#电影Title转数字字典
	title_set = set()
	for val in movies['Title'].str.split():
		title_set.update(val)

	title_set.add('<PAD>')
	title2int = {val:ii for ii, val in enumerate(title_set)}

	#将电影Title转成等长数字列表，长度是15
	title_count = 15
	title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}

	for key in title_map:
		for cnt in range(title_count - len(title_map[key])):
			title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])

	movies['Title'] = movies['Title'].map(title_map)

	#读取评分数据集
	ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']
	ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')
	ratings = ratings.filter(regex='UserID|MovieID|ratings')

	#合并三个表
	data = pd.merge(pd.merge(ratings, users), movies)

	#将数据分成X和y两张表
	target_fields = ['ratings']
	features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]

	features = features_pd.values
	targets_values = targets_pd.values

	return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig
if __name__=='__main__':
	#title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()
	#pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))
	title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))
	print(users.head())
	print(movies.head())
	print(movies.values[0])
```
###
