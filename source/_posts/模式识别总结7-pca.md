---
title: 模式识别总结7_pca
date: 2018-05-22 13:52:28
tags: [模式识别]
categories: [课堂笔记]
---
## pca的用途
一个模式识别系统设计的好坏，首要取决于所选用的特征是否较好的反映了正在研究的问题。模式识别问题的第一步是获取特征，获取来的特征被称作原始特征，其中可能有很多特征与我们研究的问题没多大关系，它们在后续的分类中甚至还有可能影响分类性能；另外，就算这些特征都是与研究问题有联系的，但是太多的特征会导致计算量大、推广能力差，所以原始特征必须要进一步清洗得到二次特征，即在保证分类器效果的前提下应该使特征数尽可能的少，主要有两种办法：
- 一种是特征选择，从D个特征中选出d(d小于D)个特征
- 另一种是特征提取或特征变换，通过把特征空间降维变换得到d个特征。

关于特征选择，需要回答两方面的问题：
- 特征的评价准则：如何衡量一组特征对分类的有效性
- 特征的寻优算法：怎样更快地找到性能最优或较好的特征组合。

关于特征提取，通常采用线性变换:$y=W^Tx$；一般情况下的特征变换就是降维变换，即高维空间到低维空间的映射。
**主成分分析(Principle Component Analysis)** 最早被提出是用于数据分析上的一种方法，其主要思想就是从一组特征中计算出一组按贡献程度从大到小排列的新特征，这组新特征是原始特征的线性组合，且相互之间不相关。

是通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。通俗的讲就是将分布在多个维度的高维数据投射到几个轴上。
## pca算法
1、假设由数据的特征和记录构成二维矩阵$X$，即$X$的一列表示一个特征，一行表示一条记录(一个示例)，$X$是一个$m$行$n$列的矩阵(需要进行减去平均值处理)。

2、计算$X$的转置$X^T$。$X^T$为$n$行$m$列的矩阵。

3、计算$X^T$任意两行之间的协方差，得到一个$n$行$n$列的协方差矩阵$CovX$。

4、求$CovX$的特征值和特征向量，得到$n$个特征值和一个$n$行$n$列的特征向量矩阵$V_0$。

5、根据$n$个特征值的大小，降序排序，取最大的$k$个特征值，并取这$k$个特征值对应的特征向量，得到一个$k$行$n$列的特征向量矩阵$V$。

6、将$k$行$n$列的特征向量$V$与$n$行$m$列的矩阵$X^T$相乘，得到$k$行$m$列的矩阵$Y_0$。

7、将$Y_0$进行转置就得到$m$行$k$列的矩阵$Y$，这个矩阵$Y$就是包含$k$个主要成分的数据。
算法代码如下：
```python
def pca(dataMat,topNfeat=9999999):
	meanVals=mean(dataMat,axis=0)  ##axis的值表示哪一维度压缩为1.=0时表示输出一行，每一列的平均值
	meanRemoved=dataMat-meanVals   ##减去平均值
	covMat=cov(meanRemoved,rowvar=0)  ##协方差
	eigVals,eigVects=linalg.eig(mat(covMat))  ##特征值和特征向量
	eigValInd=argsort(eigVals)    ##特征值排序从小到大
	eigValInd=eigValInd[:-(topNfeat+1):-1]   
	redEigVects=eigVects[:,eigValInd]   ##前topNfeat个特征及特征向量
	lowDDataMat=meanRemoved*redEigVects  
	reconMat=(lowDDataMat*redEigVects.T)+meanVals
	return lowDDataMat,reconMat
```
