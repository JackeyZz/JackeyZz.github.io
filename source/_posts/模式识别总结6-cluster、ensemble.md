---
title: 模式识别总结6_cluster、ensemble
date: 2018-05-22 13:52:03
tags: [模式识别]
categories: [课堂笔记]
---
`前言`
>  聚类分析的基本思想是 “物以类聚、人以群分”，因此大量的数据集中必然存在相似的数据点，基于这个假设就可以将数据区分出来，并发现每个数据集(分类)的特征。
> ensemble集成学习，训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。
<!--more-->
***************

## 聚类clustering
### 监督学习vs非监督学习
- 监督学习：在那些数据属性特征与目标类别属性相联系的数据集中发现其中的模式关系，这种模式关系被用来预测测试集数据的目标属性值
- 非监督学习：没有目标属性

### 聚类基础概念
**聚类** 是数据挖掘中的基本任务，聚类是将大量数据集中具有“相似”特征的数据点划分为统一类别，并最终生成多个类的方法。

聚类分析的基本思想是 **“物以类聚、人以群分”**，因此大量的数据集中必然存在相似的数据点，基于这个假设就可以将数据区分出来，并发现每个数据集(分类)的特征。

与聚类的概念类似的另外一个概念是“分类”，实际上二者经常被混用。但二者根本上是不同的：
- 学习方式不同。聚类是一种非监督式学习算法，而分类是监督式学习算法。
- 对源数据集要求不同。聚类不要求源数据集有标签，但分类需要标签用来做学习。
- 应用场景不同。聚类一般应用于做数据探索性分析，而分类更多的用于预测性分析。
- 解读结果不同。聚类算法的结果是将不同的数据集按照各自的典型特征分成不同类别，不同人对聚类的结果解读可能不同；而分类的结果却是一个固定值，不存在不同解读的情况。

聚类是一类成熟且经典的数据挖掘和机器学习算法，按照不同的维度划分有很多经典的算法，如 **K均值(K-Means)**、两步聚类、Kohonen等，甚至Python提供了9种聚类算法。

### 分区聚类(Partitional Clustering)
点到聚类$i$中心的距离：
$$se_{k_i}=\sum_{x_j\in Cluster\quad i}||x_j-C_i||^2$$
目标函数：
$$se_k=\sum_{i=1}^kse_{k_i}$$

#### K均值(K-Means)
聚类首先面临的一个问题是，如何衡量不同数据之间的“相似度”。大多数“相似度”都是基于距离计算的，距离计算可以分为两类：
- 基于几何距离的“相似度”
为了方便计算，假设平面上两点a(x1,y1)与b(x2,y2)）
  - 欧式距离 : a和b平方和的开方(结果越大，相似度越高)
$$d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$
  - 曼哈顿距离 : 每个维度的距离之和(结果越大，相似度越高)
$$d_{12}=|x_1-x_2|+|y_1-y_2|$$
  - 切比雪夫距离: 每个维度上距离的最大值(结果越大，相似度越高)
$$d_{12}=max(|x_1-x_2|,|y_1-y_2|)$$
- 基于非几何距离的“相似度”
  - 余弦距离：两个向量的夹角
$$cos\theta=\dfrac{x_1x_2+y_1y_2}{\sqrt{x_1^2+y_1^2}\sqrt{x_2^2+y_2^2}}$$
夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。
  - 汉明距离
两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。
  - 杰卡德相似系数(Jaccard similarity coefficient)
$$J(A,B)=\dfrac{|A\bigcap B|}{|A\bigcup B|}$$
  - 相关系数(Correlation coefficient)
$$\rho_{XY}=\dfrac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}=\dfrac{E((X-EX)(Y-EY))}{\sqrt{D(X)}\sqrt{D(Y)}}$$
相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1(正线性相关)或-1(负线性相关)。

除了上述常用相似度计算方法外，还可能包括马氏距离、闵可夫斯基距离、标准欧氏距离等。

回到本文的主题 **K-Means算法** 来，通过上述算法来计算数据相似度(大多数选择欧氏距离)，但我们会发现不同维度间由于度量单位的差异，计算的结果值会产生很大差异。例如假设A和B两个点分别具有两个维度：订单金额和转化率，那么订单金额的取值范围可能是0到无穷大，而转化率的取值为0到1，因此计算结果会由于订单金额的取值范围而“片面”夸大了这一维度的计算比重。所以大多数情况下，如果存在这种度量量级的差异会选择数据标准化，使不同维度落到相同的数据区间内然后进行计算。

>上面说到大多数情况下需要标准化，就意味着不是所有的场景下都需要标准化，一种是原有的数据维度之间的量级差异不大，因此没有必要标准化；二是标准化后的结果在解读时会出现难以还原的问题，例如原本均值为180在标准化后可能就是0.31，而这个数据很难还原，只能进行相对其他维度的解读。

**K-Means的计算步骤如下：**
- 为每个聚类确定一个初始聚类中心，这样就有K个初始聚类中心。
- 将样本集中的样本按照最小距离原则分配到最邻近聚类。
- 使用每个聚类中的样本均值作为新的聚类中心。
- 重复步骤2.3直到聚类中心不再变化。
- 结束，得到K个聚类

K-Means的计算既然作为一种经典算法，一定有很多 **优势**：
- 它是解决聚类问题的一种经典算法，简单、快速而且可以用于多种数据类型。
- 对处理大数据集，该算法是相对可伸缩和高效率的。
- 因为它的复杂度是O(n k t ) , 其中, n 是所有对象的数目，k 是簇的数目，t 是迭代的次数。通常k < <n 且t < <n 。
- 算法尝试找出使平方误差函数值最小的k个划分。当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。

但是，传统K-Means也有很多 **不足**：
- 既然基于均值计算，那么要求簇的平均值可以被定义和使用，此时字符串等非数值型数据则不适用。
- K-Means的第一步是确定k（要生成的簇的数目），对于不同的初始值K，可能会导致不同结果。
- 应用数据集存在局限性，适用于球状或集中分布数据，不适用于特殊情况数据。如下面这种非球状的数据分布就无法正确分类。
![](模式识别总结6-cluster、ensemble/pic1.png)
- 它对于“躁声”和孤立点数据是敏感的，少量的该类数据能够对平均值产生极大的影响。

#### 模糊C均值聚类FCM
FCM算法是基于对目标函数的优化基础上的一种数据聚类方法。聚类结果是每一个数据点对聚类中心的隶属程度，该隶属程度用一个数值来表示。FCM算法是一种无监督的模糊聚类方法，在算法实现过程中不需要人为的干预。这种算法的不足之处:首先，算法中需要设定一些参数，若参数的初始化选取的不合适，可能影响聚类结果的正确性;其次，当数据样本集合较大并且特征数目较多时，算法的实时性不太好。
**目标函数：**
$$J=\sum_{i=1}^C\sum_{k=1}^N(u_{i,k})^md_{i,k}^2\quad s.t.\sum_{i=1}^Cu_{i,k}=1$$
**聚类中心迭代公式：**
$$V_i=\frac{\sum_{k=1}^N(u_{i,k})^mx_k}{\sum_{k=1}^N(u_{i,k})^m}$$
**每个样本$k$属于类$i$的隶属度：**
$$u_{i,k}=\dfrac{1}{\sum_{j=1}^C(\dfrac{d_{k,i}}{d_{k,j}})^{\frac{2}{m-1}}}$$
**距离公式：**
$$d_{i,k}^2=||x_k-V_i||^2$$
**算法步骤：**
- 初始化数据集
- 初始化隶属度数组
- 根据隶属度数组更新聚类中心
- 根据聚类中心更新隶属度数组
- 是否达到结束条件，没有达到则重复2-4步骤。

### 层次聚类
#### Agglomerative clustering
是一种自底而上的层次聚类方法，它能够根据指定的相似度或距离定义计算出类之间的距离.
**Dendrogram：** 依次将符合条件的类相连，最后得到使算法与数据均形象化的树状结构图。专门用来描述经层次聚类算法得到的结果。
![](模式识别总结6-cluster、ensemble/pic2.png)
**Agglomerative Clustering Algorithm**
- 1.将每一个元素单独定为一类
- 2.重复：每一轮都合并指定距离(对指定距离的理解很重要)最小的类
- 3.直到所有的元素都归为同一类

**算法步骤：**
- 将每个对象归为一类, 共得到N类, 每类仅包含一个对象. 类与类之间的距离就是它们所包含的对象之间的距离.
- 找到最接近的两个类并合并成一类, 于是总的类数少了一个.
- 重新计算新的类与所有旧类之间的距离.
- 重复第2步和第3步, 直到最后合并成一个类为止(此类包含了N个对象).

**根据步骤三的不同，Agglomerative Clustering的三种不同方法**
依据对相似度(距离)的不同定义，将Agglomerative Clustering的聚类方法分为三种：
- Single-linkage:要比较的距离为元素对之间的最小距离(两个聚类的相似度取决于两个不同簇中最近的两个点)
- Complete-linkage:要比较的距离为元素对之间的最大距离(组间距离等于两组对象之间的最大距离。)
- Group average：要比较的距离为类之间的平均距离(平均距离的定义与计算：假设有A，B两个类，A中有n个元素，B中有m个元素。在A与B中各取一个元素，可得到他们之间的距离。将nm个这样的距离相加，得到距离和。最后距离和除以nm得到A，B两个类的平均距离。)

#### divisive
是一种自顶而下的层次聚类方法
## 集成ensemble
对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。
![](模式识别总结6-cluster、ensemble/pic3.png)
目前来说，**同质个体学习器** (个体学习器都是一个种类的，例如都是决策树的，或者都是神经网络的)的应用是最广泛的，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在 **强依赖关系**，一系列个体学习器基本都需要 **串行生成**，代表算法是 **boosting系列算法**，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以 **并行生成**，代表算法是 **bagging** 和 **随机森林(Random Forest)系列算法**。

### boosting
![](模式识别总结6-cluster、ensemble/pic4.png)
从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。　　

不过有几个具体的问题Boosting算法没有详细说明。
1) 如何计算学习误差率e?
2) 如何得到弱学习器权重系数α?
3) 如何更新样本权重D?
4) 使用何种结合策略？

#### AdaBoost
##### 分类问题算法
输入为样本集$T={(x_1,y_1),(x_2,y_2),...(x_m,y_m)}$，输出为{-1, +1}，弱分类器算法, 弱分类器迭代次数$K$。输出为最终的强分类器$f(x)$

初始化样本集权重为
$$D(1)=(w_{11},w_{12},...w_{1m});w_{1i}=\frac{1}{m};i=1,2...m$$
对于$k=1,2,...K$:
- 使用具有权重$D_k$的样本集来训练数据，得到弱分类器$G_k(x)$
- 计算Gk(x)的分类误差率
$$e_k=P(G_k(x_i)≠y_i)=\sum_{i=1}^mw_{ki}I(G_k(x_i)≠y_i)$$
- 计算弱分类器的系数
$$α_k=\frac{1}{2}log\frac{1-e_k}{e_k}$$
- 更新样本集的权重分布
$$w_{k+1,i}=\frac{w_{ki}}{Z_K}exp(-α_ky_iG_k(x_i))\quad i=1,2,...m$$
这里$Z_k$是规范化因子
$$Z_k=\sum_{i=1}^mw_{ki}exp(-α_ky_iG_k(x_i))$$

构建最终分类器为：
$$f(x)=sign(\sum_{k=1}^Kα_kG_k(x))$$
　　　　
对于Adaboost多元分类算法，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数
$$α_k=\frac{1}{2}log\frac{1-e_k}{e_k}+log(R-1)$$
其中R为类别数。从上式可以看出，如果是二元分类，R=2，则上式和我们的二元分类算法中的弱分类器的系数一致。

##### 回归问题算法
输入为样本集$T={(x_1,y_1),(x_2,y_2),...(x_m,y_m)}$，弱学习器算法, 弱学习器迭代次数$K$。输出为最终的强学习器$f(x)$
初始化样本集权重为
$$D(1)=(w_{11},w_{12},...w_{1m});w_{1i}=\frac{1}{m};i=1,2...m$$
对于$k=1,2,...K$:
- 使用具有权重$D_k$的样本集来训练数据，得到弱学习器$G_k(x)$
- 计算训练集上的最大误差
$$E_k=max|y_i-G_k(x_i)|i=1,2...m$$
- 计算每个样本的相对误差:
  - 如果是线性误差，则$e_{ki}=\frac{|y_i-G_k(x_i)|}{E_k}$；
  - 如果是平方误差，则$e_{ki}=\frac{(y_i-G_k(x_i))^2}{E^2_k}$
  - 如果是指数误差，则$e_{ki}=1-exp(\frac{-y_i+G_k(x_i))}{E_k})$　　　　　　　　
- 计算回归误差率
$$e_k=\sum_{i=1}^mw_{ki}e_{ki}$$
- 计算弱学习器的系数
$$α_k=\frac{e_k}{1-e_k}$$
- 更新样本集的权重分布为
$$w_{k+1,i}=\frac{w_{ki}}{Z_k}α_k^{1-e_{ki}}$$

这里$Z_k$是规范化因子
$$Z_k=\sum_{i=1}^mw_{ki}α_k^{1-e_{ki}}$$

构建最终强学习器为：
$$f(x)=\sum_{k=1}^K(ln\frac{1}{αk})g(x)$$
其中，$g(x)$是所有$α_kG_k(x),k=1,2,....K$的中位数。　　　　

##### Adaboost算法的正则化
为了防止Adaboost过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为 **步长(learning rate)**。定义为$ν$,对于前面的弱学习器的迭代
$$f_k(x)=f_{k-1}(x)+α_kG_k(x)$$
如果我们加上了正则化项，则有
$$f_k(x)=f_{k-1}(x)+να_kG_k(x)$$
$ν$的取值范围为$0<ν≤1$。对于同样的训练集学习效果，较小的$ν$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。
#### 梯度提升树GBDT
在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y,f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y,f_t(x)=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。

GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。

从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？
##### GBDT的负梯度拟合
在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第$t$轮的第$i$个样本的损失函数的负梯度表示为
$$r_{ti}=-[\frac{∂L(y_i,f(x_i))}{∂f(x_i)}\rbrack_{f(x)}=f_{t-1}(x)$$

利用$(x_i,r_{ti})(i=1,2,..m)$,我们可以拟合一颗CART回归树，得到了第$t$颗回归树，其对应的叶节点区域$R_{tj},j=1,2,...,J$。其中$J$为叶子节点的个数。

针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值$c_{tj}$如下：

$$c_{tj}=\underbrace{arg\;min}_{c}\sum_{x_i\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)$$

这样我们就得到了本轮的决策树拟合函数如下：
$$h_t(x) = \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$

从而本轮最终得到的强学习器的表达式如下：
$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$

通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。

##### GBDT回归算法
好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度。

输入是训练集样本$T={(x_1,y_1),(x_2,y_2),...(x_m,y_m)}$，最大迭代次数$T$, 损失函数$L$。输出是强学习器$f(x)$
初始化弱学习器
$$f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)$$

对迭代轮数$t=1,2,...T$有：
- 对样本$i=1,2,...m$，计算负梯度
$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg\rbrack_{f(x) = f_{t-1}\;\; (x)}$$

- 利用$(x_i,r_{ti})(i=1,2,..m)$, 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为$R_{tj},j=1,2,...,J$。其中J为回归树t的叶子节点的个数。

- 对叶子区域$j=1,2,..J$,计算最佳拟合值
$$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$

- 更新强学习器
$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$
得到强学习器f(x)的表达式

$$f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$

##### GBDT分类算法
GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。

###### 二元GBDT分类算法
对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：
$$L(y, f(x)) = log(1+ exp(-yf(x)))$$
其中$y \in\{-1, +1\}$。则此时的负梯度误差为
$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg\rbrack_{f(x) = f_{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为

$$c_{tj}=\underbrace{arg\; min}_{c}\sum_{x_i\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))$$

由于上式比较难优化，我们一般使用近似值代替
$$c_{tj}=\dfrac{\sum_{x_i\in R_{tj}}r_{ti}}{\sum_{x_i\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}$$

除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。

###### 多元GBDT分类算法
多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为$K$，则此时我们的对数似然损失函数为：
$$L(y, f(x)) = -  \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$
其中如果样本输出类别为$k$，则$y_k=1$。第$k$类的概率$p_k(x)$的表达式为：
$$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K} exp(f_l(x))$$
集合上两式，我们可以计算出第$t$轮的第$i$个样本对应类别$l$的负梯度误差为
$$r_{til} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$$
观察上式可以看出，其实这里的误差就是样本$i$对应类别$l$的真实概率和$t-1$轮预测概率的差值。
对于生成的决策树，我们各个叶子节点的最佳残差拟合值为
$$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$
由于上式比较难优化，我们一般使用近似值代替
$$c_{tjl} =  \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$
除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。

##### GBDT常用损失函数
对常用的GBDT损失函数做一个总结。
对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:
1) 如果是指数损失函数，则损失函数表达式为
$$L(y, f(x)) = exp(-yf(x))$$
其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理。
2) 如果是对数损失函数，分为二元分类和多元分类两种。

对于回归算法，常用损失函数有如下4种:
1) 均方差，这个是最常见的回归损失函数了
$$L(y, f(x)) =(y-f(x))^2$$
2) 绝对损失，这个损失函数也很常见
$$L(y,f(x))=|y-f(x)|$$
对应负梯度误差为：
$$sign(y_i-f(x_i))$$
3) Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：
$$L(y, f(x))= \begin{cases} \frac{1}{2}(y-f(x))^2& {|y-f(x)| \leq \delta}\\ \delta(|y-f(x)| - \frac{\delta}{2})& {|y-f(x)| > \delta} \end{cases}$$　　　　
对应的负梯度误差为：
$$r(y_i, f(x_i))= \begin{cases} y_i-f(x_i)& {|y_i-f(x_i)| \leq \delta}\\ \delta sign(y_i-f(x_i))& {|y_i-f(x_i)| > \delta} \end{cases}$$
4) 分位数损失。它对应的是分位数回归的损失函数，表达式为
$$L(y, f(x)) =\sum\limits_{y \geq f(x)}\theta|y - f(x)| + \sum\limits_{y < f(x)}(1-\theta)|y - f(x)|$$
其中$θ$为分位数，需要我们在回归前指定。对应的负梯度误差为：
$$r(y_i, f(x_i))= \begin{cases} \theta& { y_i \geq f(x_i)}\\ \theta - 1 & {y_i < f(x_i) } \end{cases}$$
对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。

##### GBDT的正则化
和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。
第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代
$$f_k(x)=f_{k-1}(x)+h_k(x)$$
如果我们加上了正则化项，则有
$$f_k(x)=f_{k-1}(x)+νh_k(x)$$
ν的取值范围为$0<ν≤1$。对于同样的训练集学习效果，较小的$ν$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。

第二种正则化的方式是通过子采样比例(subsample)。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。

使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。

第三种是对于弱学习器即CART回归树进行正则化剪枝。

### bagging
![](模式识别总结6-cluster、ensemble/pic5.png)
其特点即为：**随机采样**
随机采样(bootstrap)就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，一般会随机采集和训练集样本数$m$一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有$m$个样本训练集做$T$次的随机采样，，则由于随机性，$T$个采样集各不相同。

对于一个样本，它在某一次含$m$个样本的训练集的随机采样中，每次被采集到的概率是$\dfrac{1}{m}$，不被采集到的概率为$1-\dfrac{1}{m}$。如果$m$次采样都没有被采集中的概率是$(1-\dfrac{1}{m})^m$。当m→∞时，$(1-\dfrac{1}{m})^m$→$\dfrac{1}{e}≃0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中，这部分数据称之为 **袋外数据OOB**。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是 **决策树和神经网络。**

bagging的集合策略也比较简单，对于分类问题，通常使用 **简单投票法**，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用 **简单平均法**，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。

由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。

**装袋法的算法步骤：**
- 从原始样本集中抽取训练集.每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）.共进行k轮抽取，得到k个训练集.（k个训练集相互独立）
- 每次使用一个训练集得到一个模型，k个训练集共得到k个模型.（注：根据具体问题采用不同的分类或回归方法，如决策树、神经网络等）
- 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果.

### 随机森林Random Forest,RF
**与bagging的不同之处：**
- 首先，RF使用了CART决策树作为弱学习器，这让我们想到了梯度提示树GBDT。
- 第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于$n$，假设为$n_{sub}$，然后在这些随机选择的$n_{sub}$个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　　　　

如果$n_{sub}=n$，则此时RF的CART决策树和普通的CART决策树没有区别。$n_{sub}$越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说$n_{sub}$越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的$n_{sub}$的值。

#### extra trees
extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：
- 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。
- 在选定了划分特征后，RF的决策树会基于信息增益，基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。

从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是偏倚相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好。

#### Totally Random Trees Embedding
Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。

TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征x划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为$(0,1,0,0,0,\quad0,0,1,0,0,\quad0,0,0,0,1)$, 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。

映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。

#### Isolation Forest
Isolation Forest(以下简称IForest)是一种异常点检测的方法。它也使用了类似于RF的方法来检测异常点。

对于在T个决策树的样本集，IForest也会对训练集进行随机采样,但是采样个数不需要和RF一样，**对于RF，需要采样到采样集样本个数等于训练集个数**。但是IForest不需要采样这么多，一般来说，采样个数要远远小于训练集个数？为什么呢？因为我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。

对于每一个决策树的建立， IForest采用随机选择一个划分特征，对划分特征随机选择一个划分阈值。这点也和RF不同。

另外，IForest一般会选择一个比较小的最大决策树深度max_depth,原因同样本采集，用少量的异常点检测一般不需要这么大规模的决策树。

对于异常点的判断，则是将测试样本点$x$拟合到$T$颗决策树。计算在每颗决策树上该样本的叶子节点的深度$h_t(x)$。，从而可以计算出平均高度$h(x)$。此时我们用下面的公式计算样本点$x$的异常概率:
$$s(x,m)=2^{-\frac{h(x)}{c(m)}}$$
其中，$m$为样本个数。$c(m)$的表达式为：
$$c(m)=2ln(m-1)+ξ-2\frac{m-1}{m}$$
$ξ$为欧拉常数
$s(x,m)$的取值范围是[0,1],取值越接近于1，则是异常点的概率也越大。

### 结合策略
假定我得到的$T$个弱学习器是${h_1,h_2,...h_T}$
#### 平均法
对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。
- 最简单的平均是算术平均，也就是说最终预测是
$$H(x)=\dfrac{1}{T}\sum_1^Th_i(x)$$
- 如果每个个体学习器有一个权重$w$，则最终预测是
$$H(x)=\sum_{i=1}^Tw_ih_i(x)$$
- 其中$w_i$是个体学习器$h_i$的权重，通常有
$$w_i≥0,\sum_{i=1}^Tw_i=1$$

#### 投票法
对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是${c_1,c_2,...c_K}$,对于任意一个预测样本$x$，我们的$T$个弱学习器的预测结果分别是$(h_1(x),h_2(x)...h_T(x))$。
- 最简单的投票法是 **相对多数投票法**，也就是我们常说的少数服从多数，也就是$T$个弱学习器的对样本$x$的预测结果中，数量最多的类别$c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
- 稍微复杂的投票法是 **绝对多数投票法**，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。
- 更加复杂的是 **加权投票法**，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。

#### 学习法
上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了 **学习法** 这种方法，对于学习法，代表方法是 **stacking**，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
在这种情况下，我们将弱学习器称为 **初级学习器**，将用于结合的学习器称为 **次级学习器**。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。
